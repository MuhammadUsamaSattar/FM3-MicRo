- **34d2205:** Updated videos in media/Test and added log files
- **79afb3a:** Modified README.md
- **fdcd969:** Copy of project to GitHub
- **6a8d461:** Updated CHANGELOG.md
- **2e05ac9:** Removed LFS
- **a96a0e0:** Removed helper script
- **afeedf5:** Completed repo.
- **bc431d8:** Attempted to convert .json reward maps to git lfs. Added camera_pygame.py to test out the trained RL model on the experimental setup. npz_plotter.py can now take multiple files and plot them on the same plot.
- **70883b2:** Modified gui.py to interface with gym environment
- **2d399db:** Added reward maps .yaml file to git lfs storage
- **98f675f:** Data buffer contains only contains a single datapoint to prevent VRAM overflow in reward_map.py. Removed bug that treated the list instead of the str in the list as the output of the llm for cases where explanation was also outputted in rl.py and SingleParticleNoCargo.py. Fixed repeated sentence in llm_prompt_binar_rewards_1_example_explanation.yaml. Modified few scripts to run for only failed sims, these will be corrected in the next commits.
- **b325802:** Added plot saving functionality to reward_map.py
- **6787067:** Added reward_map.py script that creates the reward map and plots it
- **a43f8cc:** Missed 'System' folder in previous commit
- **b18cba5:** Created scripts for slurm based batch execution. Position calculation is done using the framerate and not actual time.
- **3552754:** Added argument for setting LLM model
- **0d490e2:** Modified npz plotter to have minimum value of 0 in y axis
- **24be5fb:** Sorted command line arguments and added .bat file to run the script with various parameters.
- **166779d:** Modified rl.py to work with command line arguments.
- **a5c358f:** Observation space consits of n particle locations rather than one. Added EvalCallback. Moved data plotting scripts to src/FM3_MicRo/data_plotting_scripts.
- **185e3b4:** Added options for the reinforcement learning file. Added prompts for continous reward values
- **d7f44c1:** Implemented both standard reward and foundation model
- **ad6a278:** Added data logging and plot generation in rl_fm_rewards.py
- **1ea3517:** Added set_reward_params() to SingleParticleNoCargo class which allows switching between FM based rewards and Euclidian distance based rewards for faster frame times during model testing
- **6a6ddfe:** Added time logging to SingleParticleNoCargo class via 'verbose' parameter
- **d22f07f:** RL with FM rewards works in sequential computing
- **887a2cc:** Started modifying gym environment to work with foundation models
- **ed1f5e3:** Prompt file is now a parameter to the ZeroShotRaw class
- **108941f:** Change VLM and LLM classes to take model_id in __init__()
- **5cf05d1:** Add additional prompts for llm based raw one shot
- **78969f3:** Change prompt output to JSON format. Change file names from 'x_shot_raw' to 'raw_x_shot'
- **9a34e58:** Implement zero_shot_raw with gym env
- **51ad47f:** Fix LLAMA chat format. Both LLAMA and LLAVA now only generate output text without input prompt
- **5fe228f:** Added comments to files and organized imports
- **e11e757:** Changed vlm class to allow both url and image as array/PIL option
- **77af1eb:** Setup vlm and llm classes along with optimizations to maximize token generation rate.
- **d151eca:** Setup llm module, vlm module and dev-scripts
- **ae0c056:** Renamed gymasium env package folder
- **244774d:** Fourth attempt to fix GitHub pusher. Set the GitHub credentials for both GitHub and GitLab
- **6ecebc7:** Third attempt to fix GitHub pusher email
- **af4993b:** Second try to correct username for github repo
- **9801377:** Insignificant change to see if the email pushing to GitHub is correct
- **e60b6bb:** Insignificant change to check if gitlab and github are synced
- **d498f89:** Changed gym env name to FM3-MicRo. Restructured entire package according to package format on python website. Added all project dependencies to the pyproject files.
- **5fb013a:** Initial commit
